{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas and Enhancements to Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readDat(fname):\n",
    "    '''\n",
    "    :param fname: input file\n",
    "    Assumes a first comment line is attribute names\n",
    "    Other rows are attribute values\n",
    "    :returns: numpy array of data, one example per row.\n",
    "    '''\n",
    "    with open(fname,'r') as fd:\n",
    "        dat = []\n",
    "        attribs = None\n",
    "        for line in fd.readlines():\n",
    "            if line[0] == '#': #ignore comments\n",
    "                attribs = line.split()[1:]\n",
    "            else:\n",
    "                x = line.split()\n",
    "                vals = [ float(val) for val in x ]\n",
    "                dat.append( vals ) # convert to floats and append\n",
    "    return attribs,np.array(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr,dat = readDat('auto.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Pandas to looks at your data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Look at data\n",
    "auto_dataframe = pd.DataFrame(dat,columns=attr)\n",
    "pd.plotting.scatter_matrix(auto_dataframe,figsize=(15,15),marker='0',hist_kwds={'bins':20},s=60,alpha=0.8);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's fit MPG vs. WEIGHT using linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(dat[:,3],dat[:,-1])\n",
    "plt.title('MPG vs. WEIGHT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradStep(dat,p):\n",
    "    '''\n",
    "    Calculate one update of the parameters in p via gradient descent\n",
    "    algorithm.\n",
    "    dat: all input data\n",
    "    p: parameters of linear equation to be adjusted\n",
    "    '''\n",
    "    sum = np.zeros(2)\n",
    "    # Variables are x = x[0:2], with x[0] always equal to 1.\n",
    "    for x in dat:\n",
    "        sum += (np.dot(pmat,x[0:2]) - x[2]) * x[0:2]\n",
    "    return sum / len(dat)\n",
    "\n",
    "\n",
    "def cost(dat,p):\n",
    "    '''Cost function (sum of squares of differences)'''\n",
    "    sum = 0\n",
    "    for x in dat:\n",
    "        sum += ( np.dot(x[0:2], p) - x[2] )**2\n",
    "    return sum / (2.0 * len(dat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradStepStoch(dat,p,alpha):\n",
    "    '''\n",
    "    Calculate one update of the parameters in p via stochastic gradient descent\n",
    "    algorithm.\n",
    "    dat: all input data\n",
    "    p: parameters of linear equation to be adjusted\n",
    "    '''\n",
    "    # For all data items sum term, (h(x) - y)*x \n",
    "    # Predicted value is y = x[2]\n",
    "    # Variables are x = x[0:2], with x[0] always equal to 1.\n",
    "    for x in dat:\n",
    "        # It's critical to separate calculation of diff\n",
    "        # from changes to parameters p!\n",
    "        diff = np.dot(p,x[0:2]) - x[2]\n",
    "        p[0] += -alpha * diff * x[0]\n",
    "        p[1] += -alpha * diff * x[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regress(dat,limit=0.001,alpha=1e-8):\n",
    "    '''Use linear regression to predict col2 from col1 in dat'''\n",
    "    p = np.random.rand(2) - 0.5   # two random values ((these are the parameters to determine)\n",
    "   \n",
    "    newp = p.copy()\n",
    "    \n",
    "    err = cost(dat,p)\n",
    "    print(err)\n",
    "    olderr = 2 * err\n",
    "\n",
    "    while olderr - err > limit: # terminate when error change is small\n",
    "        gradStepStoch(dat,p,alpha) # calculate gradient step\n",
    "        olderr = err\n",
    "        err = cost(dat,p) # determine cost (error)\n",
    "    print('Error',err,olderr)\n",
    "    print ('Parameters: ', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up input data for two variable regression by prepending BIAS\n",
    "nrows = len(dat[:,[3,-1]])\n",
    "bias = np.ones((nrows,1))\n",
    "regressDat = np.concatenate( (bias,dat[:,[3,-1]]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regress(regressDat,1e-3,1e-9) # How do the estimated parameters look?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = len(regressDat)\n",
    "x = regressDat[:,1].reshape((nrows,1))\n",
    "y = regressDat[:,2].reshape((nrows,1))\n",
    "lr = LinearRegression().fit(x,y)\n",
    "m = lr.coef_\n",
    "b = lr.intercept_\n",
    "print(lr.intercept_,lr.coef_) # intercept and slope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the data and the obtained line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= regressDat[:,1]\n",
    "plt.scatter(x,regressDat[:,2])\n",
    "plt.plot(x, 46.2 + -0.0076*x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We clearly have a problem, right?\n",
    "\n",
    "Let's clean up our data so that all variables have the same scale.  When variables are of different scales, the derivative of one can completely dominate the contribution of the other variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it stands, some variables like weight have large values, whereas others, like the number of cylinders, have small values.  We can put all variables on equal footing if we convert them to z scores $z = \\frac{(x-\\mu)}{\\sigma}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from numpy.linalg import inv\n",
    "\n",
    "class rescale:\n",
    "    '''Rescale vars and provide inverse.\n",
    "        Assumes zeroth column is all ones, the bias.\n",
    "    '''\n",
    "    def __init__(self,dat):\n",
    "        self.dat = dat\n",
    "        self.means = dat.mean(axis=0)\n",
    "        self.stdevs = dat.std(axis=0)\n",
    "        if 0 in self.stdevs[1:]:\n",
    "            raise Exception(\"Zero stdev does not permit transform.\")\n",
    "        \n",
    "    def scale(self):\n",
    "        '''Return the data columns scaled to each have \n",
    "        zero mean and stdev of 1.  The zeroth column is\n",
    "        not scaled.\n",
    "        '''\n",
    "        self.scaled = preprocessing.scale(self.dat)\n",
    "        self.scaled[:,0] = self.dat[:,0] # Copy back the bias column\n",
    "        return self.scaled\n",
    "    \n",
    "    def transform(self,x):\n",
    "        '''Transform x[1:] to new zscore.  Does not change x[0].'''\n",
    "        z = np.zeros(len(x))\n",
    "        z[0] = x[0]\n",
    "        for i in range(1,len(x)):\n",
    "            z[i] = (x[i] - self.means[i]) / self.stdevs[i]\n",
    "        return z\n",
    "                    \n",
    "    def untransform(self,z):\n",
    "        '''Transform z to score in original space.  Does not change z[0].'''\n",
    "        x = np.zeros(len(z))\n",
    "        x[0] = z[0]\n",
    "        for i in range(1,len(z)):\n",
    "            x[i] = self.means[i] + (self.stdevs[i] * z[i])\n",
    "        return x\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = rescale(regressDat)\n",
    "scaledDat = scaler.scale()\n",
    "print('means',np.mean(scaledDat,axis=0))\n",
    "print('deviations',np.std(scaledDat,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check with SciKitlearn\n",
    "nrows = len(regressDat)\n",
    "x = scaledDat[:,1].reshape((nrows,1))\n",
    "y = scaledDat[:,2].reshape((nrows,1))\n",
    "lr = LinearRegression().fit(x,y)\n",
    "m = lr.coef_\n",
    "b = lr.intercept_\n",
    "print('b and m',b,m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regress(scaledDat,1e-10,1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "The values we derive are in the renormalized space.  When testing data, we must preserve the mean and standard deviation of the __training data__ so we can renormalize the testing data in the same manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "When we fit numerous parameters, we would like to avoid overfitting and we may believe that some parameters are more important predictors than others.  Looking at the scatterplots above, which predictors have the most obvious relationships to the MPG we want to predict?\n",
    "\n",
    "One approach to this problem is to enforce a constraint on all parameters that we add to the loss function , $J(\\theta)$.  In particular we use\n",
    "$\n",
    "J(\\theta) = \\sum_{i=1}^{m} ((\\sum_{k=1}^{n} \\theta_k x^{(i)}_k) - y^{(i)}) + \\lambda \\sum_{k=1}^{n} \\theta_k^2\n",
    "$\n",
    "Of course, we choose a weighting value, $\\lambda$ and we must still decide whether we want to use stochastic gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rewrite our code a bit and include regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(dat,y,p,regwt=0.0):\n",
    "    '''Cost function (sum of squares of differences).\n",
    "        Implements L2 regularization\n",
    "    '''\n",
    "    sum = 0\n",
    "    for i,x in enumerate(dat):\n",
    "        sum += ( np.dot(x, p) - y[i] )**2 + regwt * np.dot(p,p)\n",
    "    return sum / (2.0 * len(dat))\n",
    "\n",
    "def gradStepStoch(dat,y,p,alpha,regwt=0.0):\n",
    "    '''\n",
    "    Calculate one update of the parameters in p via stochastic gradient descent\n",
    "    algorithm.  \n",
    "    dat: all input data\n",
    "    p: parameters of linear equation to be adjusted\n",
    "    regwt: regularization parameter for L2 regularization\n",
    "    '''\n",
    "    # For all data items sum term, (h(x) - y)*x \n",
    "    # Predicted value is y = x[-1]\n",
    "    # Variables are x = x[0:-1], with x[0] always equal to 1.\n",
    "    for i,x in enumerate(dat):\n",
    "        diff = np.dot(p,x) - y[i] \n",
    "        p += -alpha * diff * x\n",
    "        \n",
    "\n",
    "def regress(dat,y,regwt=0.0,alpha=1e-3,limit=1e-3):\n",
    "    '''Use linear regression to predict col2 from col1 in dat'''\n",
    "    nparams = np.shape(dat)[1] \n",
    "    p = np.random.rand(nparams) - 0.5 # two random values ((these are the parameters to determine)\n",
    " \n",
    "    err = cost(dat,y,p)\n",
    "    print(err)\n",
    "    olderr = 2 * err\n",
    "\n",
    "    while olderr - err > limit: # terminate when error change is small\n",
    "        gradStepStoch(dat,y,p,alpha,regwt) # calculate gradient step\n",
    "        olderr = err\n",
    "        err = cost(dat,y,p) # determine cost (error)\n",
    "    print('Error',err,olderr)\n",
    "\n",
    "    print ('Parameters: ', p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check our old problem.\n",
    "x = scaledDat[:,:-1]\n",
    "y = scaledDat[:,-1]\n",
    "regress(x,y,regwt=0.0,alpha=1e-3,limit=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we rescale ALL data\n",
    "nrows = len(dat)\n",
    "bias = np.ones((nrows,1))\n",
    "allDat = np.concatenate( (bias,dat),axis=1)\n",
    "scaler = rescale(allDat)\n",
    "allScaled = scaler.scale()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(allDat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = allScaled[:,:-1]\n",
    "y = allScaled[:,-1]\n",
    "regress(x,y,regwt=0.0,alpha=1e-4,limit=1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rwt=0.9\n",
    "regress(x,y,rwt,alpha=1e-5,limit=1e-9)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check with SciKitlearn\n",
    "nrows = len(x)\n",
    "xx = allScaled[:,1:-1]\n",
    "yy = allScaled[:,-1]\n",
    "lr = LinearRegression().fit(xx,yy)\n",
    "m = lr.coef_\n",
    "b = lr.intercept_\n",
    "print('b and m',b,m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allDat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = allDat[:,:-1]\n",
    "I = np.eye( len(X.T) )\n",
    "regwt = 0.0\n",
    "Y = allDat[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = np.matmul(X.T,X)\n",
    "Minv = np.linalg.inv(M)\n",
    "Q = np.dot(X.T,Y)\n",
    "np.dot(Minv,Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression().fit(X,Y)\n",
    "m = lr.coef_\n",
    "b = lr.intercept_\n",
    "print('b and m',b,m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
